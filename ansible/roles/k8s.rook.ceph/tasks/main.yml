- name: Add repo
  when: inventory_hostname in groups['k8sM']
  kubernetes.core.helm_repository:
    name: rook-release
    repo_url: https://charts.rook.io/release

- name: Set fact
  ansible.builtin.set_fact:
    action: "{{ 'present' if uninstall is not defined or not uninstall else 'absent' }}"

# Refs: https://github.com/rook/rook/blob/release-1.9/deploy/charts/rook-ceph/values.yaml
# ceph-volume lvm zap /dev/sda
# https://forum.proxmox.com/threads/how-to-wipe-a-disk-which-was-osd-before.59373/?fbclid=IwAR2YBNQCwUubSqH8C3OJPDAgFVMxlCdlpHwV0A5FAx5CTtCemA_Pd0J3baM
- name: "Do {{ action }} rook-ceph operator"
  when: inventory_hostname in groups['k8sM']
  kubernetes.core.helm:
    create_namespace: true
    release_namespace: rook-ceph
    chart_ref: rook-release/rook-ceph
    update_repo_cache: true
    release_state: "{{ action }}"
    release_name: rook-ceph
    release_values:
      pspEnable: false
      useOperatorHostNetwork: true
      monitoring:
        enabled: true
      resources: null
      csi:
        # forceCephFSKernelClient: false
        # enableSelinuxRelabeling: false
        # disableAdmissionController: true
        # hostpathRequiresPrivileged: true
        # RBD
        enableRbdDriver: false
        enableRBDSnapshotter: false
        csiRBDProvisionerResource: null
        csiRBDPluginResource: null

        # CephFS
        enableCephfsDriver: true
        enableCephfsSnapshotter: false
        csiCephFSPluginResource: null

        # GRpc
        enableGrpcMetrics: false

        # CSI
        csiNFSProvisionerResource: null
        csiNFSPluginResource: null

# Refs: https://github.com/rook/rook/blob/release-1.9/deploy/charts/rook-ceph-cluster/values.yaml
- name: "Do {{ action }} rook-ceph-cluster"
  when: inventory_hostname in groups['k8sM']
  kubernetes.core.helm:
    create_namespace: true
    release_namespace: rook-ceph
    chart_ref: rook-release/rook-ceph-cluster
    update_repo_cache: true
    release_state: "{{ action }}"
    release_name: rook-ceph-cluster
    release_values:
      # pspEnable: false
      monitoring:
        enabled: true
      toolbox:
        enabled: true
      cephClusterSpec:
        # cephVersion:
          # image: quay.io/ceph/ceph:v17.2.5 # Quincy
          # image: quay.io/ceph/ceph:v16.2.10 # Pacific (Default)
          # allowUnsupported: true
        mon:
          allowMultiplePerNode: true
        mgr:
          allowMultiplePerNode: true
        dashboard:
          enabled: true
          ssl: false
          port: "{{ INGRESS_NGINX_HTTP_PORT }}"
        crashCollector:
          disable: true
        resources: null
        storage:
          useAllNodes: true
          useAllDevices: true
          # deviceFilter: ^sd[a-z]+$
      cephBlockPools: null
      cephObjectStores: null

- name: Deploy Ingress
  ansible.builtin.include_role:
    name: k8s.rook.ceph
    tasks_from: ingress

- name: Fetch admin password
  ansible.builtin.shell: |
    kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
  register: result
  until: result.rc == 0
  retries: 10
  delay: 10

- name: Show admin password
  ansible.builtin.debug:
    msg:
      - "Ceph Portal: http://{{ DOMAINS['ceph'] }}"
      - " - user: admin"
      - " - pass: {{ result.stdout }}"
# Misc
# ceph mgr module enable rook
# ceph orch set backend rook

# https://www.cnblogs.com/gzxbkk/p/7737467.html
# sgdisk -n 1:0:+20G /dev/sda -t 1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -c 1:"ceph journal"
# sgdisk -n 2:0:+20G /dev/sda -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -c 2:"ceph journal"
# sgdisk -n 3:0:+800G /dev/sda -t 3:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -c 3:"ceph data"
# sgdisk -n 4:0:+800G /dev/sda -t 4:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -c 4:"ceph data"
